# ğŸ­ Facial Mood Detection

<div align="center">

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.x-red.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**Real-time facial emotion detection using Deep Learning and Computer Vision**

[Features](#-features) â€¢ [Demo](#-demo) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Model](#-model-architecture) â€¢ [Results](#-results)

</div>

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ¯ **7 Emotions** | Detects Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise |
| ğŸ§  **Transfer Learning** | Uses MobileNetV2 pre-trained on ImageNet |
| ğŸ“· **Real-time Detection** | Live webcam emotion detection |
| ğŸŒ **Web Application** | Beautiful Streamlit UI for easy use |
| âš–ï¸ **Class Balancing** | Handles imbalanced datasets effectively |
| ğŸ“Š **Confidence Scores** | Shows probability for all emotions |

---

## ğŸ¬ Demo

### Web Application
```bash
./venv/bin/streamlit run src/streamlit_app.py
```

### Real-time Webcam
```bash
./venv/bin/python src/main.py
```

---

## ğŸ“ Project Structure

```
facial_mood_detection/
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â””â”€â”€ raw/                    # FER2013 dataset (7 emotion folders)
â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”œâ”€â”€ best_model.keras        # Trained model
â”‚   â”œâ”€â”€ class_indices.txt       # Emotion class mappings
â”‚   â””â”€â”€ training_history.png    # Training visualization
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ capture_data.py         # Webcam data collection
â”‚   â”œâ”€â”€ download_data.py        # Download FER2013 dataset
â”‚   â”œâ”€â”€ model.py                # Basic CNN architecture
â”‚   â”œâ”€â”€ model_transfer.py       # MobileNetV2 transfer learning
â”‚   â”œâ”€â”€ train.py                # Basic CNN training
â”‚   â”œâ”€â”€ train_transfer.py       # Transfer learning training
â”‚   â”œâ”€â”€ main.py                 # Real-time webcam inference
â”‚   â””â”€â”€ streamlit_app.py        # Web application
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/isha056/facial_mood_detection.git
cd facial_mood_detection
```

### 2ï¸âƒ£ Create Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Download Dataset
```bash
python src/download_data.py
```

---

## ğŸ¯ Usage

### Option 1: Train the Model

**Basic CNN (faster, ~47% accuracy):**
```bash
python src/train.py
```

**Transfer Learning (recommended, ~48% accuracy):**
```bash
python src/train_transfer.py
```

### Option 2: Run Inference

**Web Application (recommended):**
```bash
streamlit run src/streamlit_app.py
```
Then open http://localhost:8501 in your browser.

**Real-time Webcam:**
```bash
python src/main.py
```
Press `q` (while webcam window is focused) to quit.

---

## ğŸ§  Model Architecture

### Transfer Learning Model (MobileNetV2)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Input (96x96x1)                       â”‚
â”‚                      Grayscale                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GrayscaleToRGB Layer                        â”‚
â”‚                (Custom Layer)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            MobileNetV2 (Pre-trained)                     â”‚
â”‚           ImageNet weights, frozen                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             GlobalAveragePooling2D                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Dense(256) â†’ Dropout(0.5) â†’ Dense(128) â†’ Dropout(0.3)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Dense(7, softmax)                           â”‚
â”‚         Output: 7 emotion probabilities                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Strategy

| Phase | Description | Learning Rate | Epochs |
|-------|-------------|---------------|--------|
| **Phase 1** | Train classification head (base frozen) | 0.001 | 15 |
| **Phase 2** | Fine-tune top 30 layers of MobileNetV2 | 0.00005 | 25 |

---

## ğŸ“Š Results

### Dataset: FER2013

| Emotion | Training Samples | Class Weight |
|---------|-----------------|--------------|
| ğŸ˜  Angry | ~4,000 | 0.889 |
| ğŸ¤¢ Disgust | ~436 | **4.074** |
| ğŸ˜¨ Fear | ~4,000 | 0.889 |
| ğŸ˜Š Happy | ~7,000 | 0.889 |
| ğŸ˜ Neutral | ~4,900 | 0.887 |
| ğŸ˜¢ Sad | ~4,800 | 0.889 |
| ğŸ˜² Surprise | ~3,200 | 0.889 |

### Model Performance

| Model | Validation Accuracy | Training Time |
|-------|---------------------|---------------|
| Basic CNN | 47.1% | ~30 min |
| CNN + Class Balancing | 42.8% | ~30 min |
| **Transfer Learning** | **47.8%** | ~20 min |

> **Note:** FER2013 is a challenging dataset. Human accuracy is only ~65-72%!

---

## ğŸ¨ Emotion Visualization

| Emotion | Emoji | Color |
|---------|-------|-------|
| Angry | ğŸ˜  | ğŸ”´ Red |
| Disgust | ğŸ¤¢ | ğŸŸ£ Purple |
| Fear | ğŸ˜¨ | ğŸŸ  Orange |
| Happy | ğŸ˜Š | ğŸŸ¢ Green |
| Neutral | ğŸ˜ | âšª Gray |
| Sad | ğŸ˜¢ | ğŸ”µ Blue |
| Surprise | ğŸ˜² | ğŸŸ¡ Yellow |

---

## ğŸ› ï¸ Technologies Used

<div align="center">

| Technology | Purpose |
|------------|---------|
| **TensorFlow/Keras** | Deep Learning Framework |
| **MobileNetV2** | Pre-trained CNN for Transfer Learning |
| **OpenCV** | Face Detection & Image Processing |
| **Streamlit** | Web Application Framework |
| **NumPy/Pandas** | Data Manipulation |
| **Matplotlib** | Training Visualization |
| **scikit-learn** | Class Weight Computation |

</div>

---

## ğŸ“ Requirements

```
opencv-python>=4.5
numpy>=1.21
pandas>=1.3
matplotlib>=3.4
tensorflow>=2.10
scikit-learn>=1.0
streamlit>=1.20
datasets>=2.0
pillow>=9.0
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘©â€ğŸ’» Author

**Isha Sharma**

- GitHub: [@isha056](https://github.com/isha056)

---

## ğŸ™ Acknowledgments

- [FER2013 Dataset](https://www.kaggle.com/datasets/msambare/fer2013) for facial expression data
- [MobileNetV2](https://arxiv.org/abs/1801.04381) for transfer learning architecture
- [Streamlit](https://streamlit.io/) for the amazing web app framework

---

<div align="center">

**â­ Star this repo if you found it helpful!**

Made with â¤ï¸ for Capstone Project

</div>